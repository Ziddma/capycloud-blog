"use strict";exports.id=214,exports.ids=[214],exports.modules={48214:n=>{n.exports=JSON.parse('[{"id":"27e4a369-535d-801b-ba5b-e493f6dc4df5","title":"Post Installation OCP on AWS (part 2)","slug":"post-installation-ocp-on-aws-part-2","coverImage":"/notion-images/796397936dd4818f222d3b72b333ebd9e81bd8dd.png","coverImageOriginal":"/notion-images/796397936dd4818f222d3b72b333ebd9e81bd8dd.png","description":"OpenShift Container Platform after deployment on AWS. This guide covers post-install checks, access setup, and best practices to ensure your OCP cluster.","date":"2025-07-18","content":"\\nOpenShift Container Platform after deployment on AWS. This guide covers post-install checks, access setup, and best practices to ensure your OCP cluster.\\n\\n\\n# Introduction\\n\\n\\nIn part one we are donely create the cluster OCP with IPI method\\n\\n\\nTopology :\\n\\n\\n![image.png](/notion-images/933f03500e2b02a291d60fa6ba737fe8bbf6c5ea.png)\\n\\n\\n# Prerequisite\\n\\n1. Account Redhat Subscription Openshift\\n2. Aws Console\\n3. Account RedHat Subscribtion\\n\\n# Post Installation\\n\\n\\n### A. Add Admin User\\n\\n\\nCreate dedicated admin user is standard post-installation step for every OCP cluster, this user serves as the primary cluster administrator, as the default \'kubeadmin\' user is temporary and should be removed according to best practices\\n\\n> [**Reference**](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/authentication_and_authorization/configuring-identity-providers#configuring-htpasswd-identity-provider)\\n1. **Create** <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`htpasswd`**</span>\\n\\n    ```bash\\n    sudo apt install apache2-utils -y\\n    htpasswd -c -B user.htpasswd __NCOLOR_START:red__adminocp__NCOLOR_END__\\n    - <Password>\\n    ```\\n\\n2. **Create** <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`secret`**</span>\\n\\n    ```bash\\n    oc create secret generic __NCOLOR_START:red__htpass-secret__NCOLOR_END__ --from-file=htpasswd=<path_to_users.htpasswd> -n openshift-config\\n    ```\\n\\n3. **Edit** **`oauth`**\\n\\n    ![image.png](/notion-images/790a8cbe8a33fe8319c6725278cf20c30ea96b3e.png)\\n\\n\\n    ```bash\\n    oc get oauth cluster -o yaml > oauth.yaml\\n    vim oauth.yaml\\n    ```\\n\\n\\n    ```yaml\\n    spec:\\n      identityProviders:\\n        - name: local-user\\n          mappingMethod: claim\\n          type: HTPasswd\\n          htpasswd:\\n            fileData:\\n              name: __NCOLOR_START:red__htpass-secret__NCOLOR_END__\\n    ```\\n\\n\\n    ```bash\\n    oc replace -f oauth.yaml\\n    # apply policy\\n    oc adm policy add-cluster-role-to-user cluster-admin __NCOLOR_START:red__adminocp__NCOLOR_END__\\n    ```\\n\\n4. Verify Login user `adminocp` or login via internal API CLI\\n\\n### B. Machine Config Pool\\n\\n\\nMachine Config Pool (MCP) works by grouping the types of infrastructure that we will create later, in part 1 there is a topology explained we will use 3 types of infrastructure nodes namely : Router, Logging, and Monreg. From each infrastructure node is template take from worker\\n\\n> [**Reference**](https://www.redhat.com/en/blog/openshift-container-platform-4-how-does-machine-config-pool-work)\\n\\nRepeat this step for <span class=\\"notion-red\\" style=\\"color:#E03A45\\">Router, Logging, Monreg</span>\\n\\n- router\\n- logging\\n- monreg\\n1. Login **Dashboard Menu →** <span class=\\"notion-blue\\" style=\\"color:#0B6E99\\">**`Compute`**</span>**→** <span class=\\"notion-blue\\" style=\\"color:#0B6E99\\">**`MachineConfigPools`**</span>**→** Create <span class=\\"notion-blue\\" style=\\"color:#0B6E99\\">**`MachineConfigPools`**</span>\\n\\n    ![image.png](/notion-images/3042e8a10607883441ee96e771901cdba632bcab.png)\\n\\n\\n    ```yaml\\n    apiVersion: machineconfiguration.openshift.io/v1\\n    kind: MachineConfigPool\\n    metadata:\\n      name: __NCOLOR_START:red__router __NCOLOR_END____NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n    spec:\\n      machineConfigSelector:\\n        matchExpressions:\\n          - key: machineconfiguration.openshift.io/role\\n            operator: In\\n            values:\\n    \\t\\t      - worker\\n              - __NCOLOR_START:red__router__NCOLOR_END____NCOLOR_START:red__  __NCOLOR_END____NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n      nodeSelector:\\n        matchLabels:\\n          node-role.kubernetes.io/__NCOLOR_START:red__router__NCOLOR_END__: \\"\\" __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n    ```\\n\\n2. Verify Machine Config Pools\\n\\n### C. Machine Sets\\n\\n\\nMachineSets in AWS OpenShift are used to automatically manage the configuration and deployment of EC2 instances. These instances are typically distributed across multiple AWS Availability Zones (AZs) based on the defined subnets to ensure high availability and fault tolerance. By default, MachineSets are used to manage worker nodes, and in a multi-zone setup. For example, using the ap-southeast-1 (Singapore) region with three AZs (a, b, and c) separate MachineSets should be defined for each zone. However, in this case, we will define MachineSets not only by zone but also by node type, separating those used for infra nodes and those for application workloads (worker nodes).\\n\\n> [<span class=\\"notion-red\\" style=\\"color:#E03A45\\">**Reference**</span>](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/machine_management/managing-compute-machines-with-the-machine-api)\\n> [<span class=\\"notion-red\\" style=\\"color:#E03A45\\">**Reference**</span>](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/machine_management/managing-compute-machines-with-the-machine-api)\\n\\n\uD83D\uDEA8 Before do configuration for machine sets, you need to identify these needs from your cluster, you need to access AWS console\\n\\n\\n<span class=\\"notion-red\\" style=\\"color:#E03A45\\">Example</span> :\\n\\n- **<infrastructure_id> = ocp-dev-rdscr**\\n- **<ami_id> =  ami-0e22aa720418e43f5**\\n1. **Dashboard Menu →** <span class=\\"notion-blue\\" style=\\"color:#0B6E99\\">**`Compute`**</span> **→** <span class=\\"notion-blue\\" style=\\"color:#0B6E99\\">**`MachineSets`**</span> **→** Create <span class=\\"notion-blue\\" style=\\"color:#0B6E99\\">**`MachineSets`**</span>\\n\\n    ![image.png](/notion-images/faab80d3c1fdf4ae5bbd22b11152e03188792ebd.png)\\n\\n\\n    \uD83D\uDCA1**Repeat this step for Worker Node & Infra Node**\\n\\n    - logging = ap-southeast-1a - ap-southeast-1b - ap-southeast-1c\\n    - monreg = ap-southeast-1a - ap-southeast-1b\\n    - router = ap-southeast-1a - ap-southeast-1c\\n    - worker = ap-southeast-1a - ap-southeast-1b - ap-southeast-1c\\n\\n    ```yaml\\n    apiVersion: machine.openshift.io/v1beta1\\n    kind: MachineSet\\n    metadata:\\n      name: __NCOLOR_START:red__<infrastructure_id>-monreg-ap-southeast-1a__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n      namespace: openshift-machine-api\\n      labels:\\n        machine.openshift.io/cluster-api-cluster: __NCOLOR_START:red__<infrastructure_id> __NCOLOR_END____NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n    spec:\\n      replicas: 1\\n      selector:\\n        matchLabels:\\n          machine.openshift.io/cluster-api-cluster: __NCOLOR_START:red__<infrastructure_id>__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n          machine.openshift.io/cluster-api-machineset: __NCOLOR_START:red__<infrastructure_id>__NCOLOR_END__-__NCOLOR_START:red__monreg-ap-southeast-1a __NCOLOR_END____NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n      template:\\n        metadata:\\n          labels:\\n            machine.openshift.io/cluster-api-cluster: __NCOLOR_START:red__<infrastructure_id>__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n            machine.openshift.io/cluster-api-machine-role: __NCOLOR_START:red__monreg __NCOLOR_END____NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n            machine.openshift.io/cluster-api-machine-type: __NCOLOR_START:red__monreg __NCOLOR_END____NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n            machine.openshift.io/cluster-api-machineset: __NCOLOR_START:red__<infrastructure_id>-monreg-ap-southeast-1a__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n        spec:\\n          metadata:\\n            labels:\\n              node-role.kubernetes.io/__NCOLOR_START:red__monreg__NCOLOR_END__: \\"\\" __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n          providerSpec:\\n            value:\\n              apiVersion: machine.openshift.io/v1beta1\\n              kind: AWSMachineProviderConfig\\n              ami:\\n                id: __NCOLOR_START:red__<ami_id>__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n              instanceType: __NCOLOR_START:red__m5.4xlarge__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n              iamInstanceProfile:\\n                id: __NCOLOR_START:red__<infrastructure_id>__NCOLOR_END__-worker-profile __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n              placement:\\n                availabilityZone: __NCOLOR_START:red__ap-southeast-1a__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n                region: __NCOLOR_START:red__ap-southeast-1__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n              securityGroups:\\n                - filters:\\n                    - name: tag:Name\\n                      values:\\n                        - __NCOLOR_START:red__<infrastructure_id>__NCOLOR_END__-node\\n                - filters:\\n                    - name: tag:Name\\n                      values:\\n                        - __NCOLOR_START:red__<infrastructure_id>__NCOLOR_END__-lb\\n              subnet:\\n                filters:\\n                  - name: tag:Name\\n                    values:\\n                      - __NCOLOR_START:red__<infrastructure_id>-private-ap-southeast-1a__NCOLOR_END__ __NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n              blockDevices:\\n                - ebs:\\n                    volumeSize: 100\\n                    volumeType: gp2\\n                    iops: 4000\\n              credentialsSecret:\\n                name: aws-cloud-credentials\\n              userDataSecret:\\n                name: worker-user-data\\n              tags:\\n                - name: kubernetes.io/cluster/__NCOLOR_START:red__<infrastructure_id> __NCOLOR_END____NCOLOR_START:green__#CHANGE THIS#__NCOLOR_END__\\n                  value: owned\\n    ```\\n\\n2. Verify Machine Sets\\n\\n### D. Project Template\\n\\n\\n\uD83D\uDCA1Project templates are used to provision projects with predefined settings. In this case, we add node selectors so that when an application project is created, workloads are automatically scheduled on designated worker nodes. To ensure this works correctly, make sure that appropriate <span class=\\"notion-green\\" style=\\"color:#0F7B58\\">**node labels have been applied to both worker and infra nodes**</span>. Additionally, you can disable self-provisioning of projects for regular users if needed.\\n\\n> [**Reference**](https://docs.redhat.com/en/documentation/openshift_container_platform/4.7/html/applications/projects#configuring-project-creation)\\n1. Create Project template\\n\\n    ```bash\\n    oc adm create-bootstrap-project-template -o yaml > yaml_collection/template.yaml\\n    vim yaml_collection/template.yaml\\n    ```\\n\\n\\n    ```yaml\\n    apiVersion: config.openshift.io/v1\\n    kind: Template\\n    metadata:\\n      creationTimestamp: null\\n      name: __NCOLOR_START:red__project-request__NCOLOR_END__\\n    objects:\\n    - apiVersion: project.openshift.io/v1\\n      kind: Project\\n      metadata:\\n        annotations:\\n          openshift.io/description: ${PROJECT_DESCRIPTION}\\n          openshift.io/display-name: ${PROJECT_DISPLAYNAME}\\n          openshift.io/requester: ${PROJECT_REQUESTING_USER}\\n          openshift.io/node-selector: __NCOLOR_START:red__node-role.kubernetes.io/worker= __NCOLOR_END____NCOLOR_START:green__#ADD THIS#__NCOLOR_END__\\n        creationTimestamp: null\\n        name: ${PROJECT_NAME}\\n      spec: {}\\n      status: {}\\n    - apiVersion: rbac.authorization.k8s.io/v1\\n      kind: RoleBinding\\n      metadata:\\n        creationTimestamp: null\\n        name: admin\\n        namespace: ${PROJECT_NAME}\\n      roleRef:\\n        apiGroup: rbac.authorization.k8s.io\\n        kind: ClusterRole\\n        name: admin\\n      subjects:\\n      - apiGroup: rbac.authorization.k8s.io\\n        kind: User\\n        name: ${PROJECT_ADMIN_USER}\\n    parameters:\\n    - name: PROJECT_NAME\\n    - name: PROJECT_DISPLAYNAME\\n    - name: PROJECT_DESCRIPTION\\n    - name: PROJECT_ADMIN_USER\\n    - name: PROJECT_REQUESTING_USER\\n    ```\\n\\n\\n    ```bash\\n    oc apply -f yaml_collection/template.yaml -n __NCOLOR_START:red__openshift-config__NCOLOR_END__\\n    ```\\n\\n2. Edit project config\\n\\n    ```bash\\n    oc edit project.config.openshift.io/cluster\\n    ```\\n\\n\\n    ![image.png](/notion-images/118250195b478508aeff8863569cb95b893d56b9.png)\\n\\n\\n    ```yaml\\n    spec:\\n      projectRequestMessage: Contact Cluster System Administrator to Create Project\\n      projectRequestTemplate: \\n        name: __NCOLOR_START:red__project-request __NCOLOR_END__\\n    ```\\n\\n3. Disable self provisioning project for basic user\\n\\n    ```bash\\n    oc patch clusterrolebinding.rbac self-provisioners -p \'{\\"subjects\\": null}\'\\n    \\n    #Disable automatic update to cluster role#\\n    oc patch clusterrolebinding.rbac self-provisioners -p \'{ \\"metadata\\": { \\"annotations\\": { \\"rbac.authorization.kubernetes.io/autoupdate\\": \\"false\\" } } }\'\\n    ```\\n\\n4. Verify Project Template\\n\\n    Verify disable self provision project\\n\\n\\n    ```bash\\n    oc new-project __NCOLOR_START:red__<project>__NCOLOR_END__ --as=__NCOLOR_START:red__<basic_user>__NCOLOR_END__ --as-group=system:authenticated --as-group=system:authenticated:oauth\\n    ```\\n\\n\\n    Expected output \\n\\n\\n    ![image.png](/notion-images/fe3b6d9494d5e20a778bb8526839e17cb7fee200.png)\\n\\n\\n    How to enable self provision project :\\n\\n\\n    ```bash\\n    oc adm policy add-cluster-role-to-group self-provisioner system:authenticated:oauth\\n    oc patch clusterrolebinding.rbac self-provisioners -p \'{ \\"metadata\\": { \\"annotations\\": { \\"rbac.authorization.kubernetes.io/autoupdate\\": \\"true\\" } } }\'\\n    ```\\n\\n\\n    Verify basic-user create project from template add node selector\\n\\n\\n    ```bash\\n    #1. Check node role#\\n    oc get node -o wide\\n    \\n    #2. Change project to test app workload#\\n    oc login -u __NCOLOR_START:red__<basic-user> __NCOLOR_END__-p__NCOLOR_START:red__ <user-password> __NCOLOR_END__\\n    oc project __NCOLOR_START:red__<test-project>__NCOLOR_END__\\n    \\n    #3. Create test app#\\n    oc new-app https://github.com/rgerardi/hellogo.git\\n    \\n    #4. Scale replica pod to make sure app only run in worker node#\\n    oc scale deployment/hellogo --replicas=10\\n    ```\\n\\n\\n    Expected output \\n\\n5. Check Node role\\n\\n    ![image.png](/notion-images/15ce003890110485d7983f80f121f17a7855860b.png)\\n\\n6. Change project to test app workload\\n\\n    ![image.png](/notion-images/415b82ecb3af14312ee4ba3eec641f99ca1bf4c6.png)\\n\\n7. Create test app\\n\\n    ![image.png](/notion-images/496a29f9117bf6c9470cb77656de29db5d5780f3.png)\\n\\n8. Scale app verify run in worker node\\n\\n    ![image.png](/notion-images/8c1f684d0ed8d47afa0beb079b3e8e861504fd2d.png)\\n\\n\\n    Delete recent testing project\\n\\n\\n    ```bash\\n    oc login -u admin -p __NCOLOR_START:red__<password-admin>__NCOLOR_END__\\n    oc delete project __NCOLOR_START:red__<test-project>\\n    __NCOLOR_END__oc delete user__NCOLOR_START:red__ <basic_user>__NCOLOR_END__\\n    ```\\n\\n\\n### E. Garbage Collection\\n\\n\\nGarbage Collection is used for automatic controlling node resource based on threeshold config by prunning unused image and current pod resources\\n\\nGarbage Collection digunakan untuk kontrol automasi penggunaan node resource dengan cara menghapus image yang tidak digunakan atau prunning image atau dengan membatasi penggunaan resource pod yang di schedule di node worker. Garbage Collection berhubungan dengan membuang image yang tidak digunakan oleh running pods.\\n\\n\\n### F. Automated ETCD backup\\n\\n\\n# Nodes Installation\\n\\n\\n### A. Router Ingress\\n\\n\\n### B. Monitoring\\n\\n\\n### C. Image Registry\\n\\n\\n### D. Logging\\n\\n","tags":["Cloud","Monitoring","DevOps"],"category":"Openshift","headings":[{"level":1,"text":"Introduction","slug":"introduction"},{"level":1,"text":"Prerequisite","slug":"prerequisite"},{"level":1,"text":"Post Installation","slug":"post-installation"},{"level":3,"text":"A. Add Admin User","slug":"a-add-admin-user"},{"level":3,"text":"B. Machine Config Pool","slug":"b-machine-config-pool"},{"level":3,"text":"C. Machine Sets","slug":"c-machine-sets"},{"level":3,"text":"D. Project Template","slug":"d-project-template"},{"level":3,"text":"E. Garbage Collection","slug":"e-garbage-collection"},{"level":3,"text":"F. Automated ETCD backup","slug":"f-automated-etcd-backup"},{"level":1,"text":"Nodes Installation","slug":"nodes-installation"},{"level":3,"text":"A. Router Ingress","slug":"a-router-ingress"},{"level":3,"text":"B. Monitoring","slug":"b-monitoring"},{"level":3,"text":"C. Image Registry","slug":"c-image-registry"},{"level":3,"text":"D. Logging","slug":"d-logging"}],"status":"Published"},{"id":"27d4a369-535d-80a8-ab04-eb6d0ad8477c","title":"Deploy OCP on AWS with IPI Method (part 1)","slug":"deploy-ocp-on-aws-with-ipi-method-part-1","coverImage":"/notion-images/b438511e0f3356c7a6a85d566277ca19887e21ef.png","coverImageOriginal":"/notion-images/b438511e0f3356c7a6a85d566277ca19887e21ef.png","description":"Easily deploy Red Hat OpenShift Container Platform (OCP) on AWS using the Installer-Provisioned Infrastructure (IPI) method.","date":"2025-03-21","content":"\\nEasily deploy Red Hat OpenShift Container Platform (OCP) on AWS using the Installer-Provisioned Infrastructure (IPI) method.\\n\\n\\n# Topology\\n\\n\\nRedHat\'s container orchestration platform with an easy-to-use IPI deployment method for AWS\\n\\n\\nTopology :\\n\\n\\n![image.png](/notion-images/416916e59bba6a44c76d2b3c0a7da44f47c05765.png)\\n\\n\\n# Prerequisite\\n\\n1. Account Subscribtion RedHat\\n    1. Login to -> [<span class=\\"notion-red\\" style=\\"color:#E03A45\\">Console RedHat</span>](https://console.redhat.com/openshift/install/aws/installer-provisioned)\\n    2. Copy Pull Secret\\n2. Account AWS Administrator\\n    1. Access key ID\\n    2. Secret Access key\\n3. Active Public Domain\\n    1. Hosted Zone AWS\\n    2. Register Propagate Nameserver\\n\\n### **A. DNS Record**\\n\\n> [Reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/installing_on_aws/installing-aws-account#installation-aws-route53_installing-aws-account)\\n\\n\uD83D\uDCA1 Use the hosted zone in the AWS menu to set up DNS for the cluster. The hosted zone will be configured with a public IP address later.\\n\\n1. **Create Hosted Zone** **`Route 53`** **→ Hosted Zone →\xa0 Create** **`Hosted Zone`**\\n\\n    ![image.png](/notion-images/837105248d9d072de5a5e235a316477ecb31cf0e.png)\\n\\n    - Domain name : **(Active Domain)**\\n    - Type : **Public hosted Zone**\\n    - **Save**\\n    - Copy All Nameserver\\n2. Choose **Active Domain** **→** <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`Nameserver`**</span> **→** Paste all nameserver from <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`Hosted Zone`**</span>\\n\\n    ![image.png](/notion-images/f87c8cc0b35c4eda52486b004488ad0e3f609cd6.png)\\n\\n    - Wait up to 24 hours for the DNS propagation to complete\\n    - Check propagation : [https://dnschecker.org/](https://dnschecker.org/)\\n\\n### B. **VPC**\\n\\n> [Reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/installing_on_aws/installer-provisioned-infrastructure#installing-aws-vpc)\\n\\n\uD83D\uDCA1 Using a custom VPC for cluster traffic isolation\\n\\n1. Create **`VPC`**\\n\\n    ![image.png](/notion-images/5b8866158b770b233e810da1a226005216966a89.png)\\n\\n\\n    ![image.png](/notion-images/26eea518dd27156087e3acf15ed72d77a8aacbd4.png)\\n\\n\\n    ![image.png](/notion-images/459c02ab54750e1549e7625193f3a23319fb9af8.png)\\n\\n    - Resource to create : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">VPC and More</span>\\n    - Name tag auto-generation : **(VPC cluster name)**\\n    - IPv4 CIDR Block : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">10.0.0.0/16</span>\\n    - Availability Zone : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">3</span>\\n    - Subnet Public : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">3</span>\\n    - Subnet Private : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">3</span>\\n    - Nat gateways : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">in 1 AZ</span>\\n    - VPC Endpoint : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">S3 Gateway</span>\\n    - **Create VPC**\\n\\n### C. **Security Group** \\n\\n\\n\uD83D\uDCA1 Security Group Needed for access traffic will be use in Bastion\\n\\n1. Create `Security Group`\\n\\n    ![2043ce9e-fdb3-4658-863b-39b56e9e0734.png](/notion-images/016caf60bb0234a9ddf77899335a36ff288806fe.png)\\n\\n    - Security Group Name : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">BastionSG</span>\\n    - Inbound : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">SSH - Source Anywhere</span>\\n    - Outbond : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">All Traffic - Source Anywhere</span>\\n\\n### D. **Bastion**\\n\\n> [Reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/networking/accessing-hosts#accessing-hosts-on-aws_accessing-hosts)\\n\\n\uD83D\uDCA1 Bastion is used to access the cluster that will be created later. The bastion also plays a role in the OCP cluster bootstrap installation.\\n\\n1. Create Bastion <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`EC2`**</span> **→** <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`Instance`**</span>\\n\\n    ![1244f1ac-d795-4ff2-bc6b-3da92fd30f21.png](/notion-images/723f8cdf338ae39276866bc5631cbad6fbe5b967.png)\\n\\n    - Name : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">Bastion</span>\\n    - AMI : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">Ubuntu server 24.04</span>\\n    - Instance Type : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">t2.2xlarge</span>\\n    - Keypair : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">ocp-key</span>\\n    - VPC : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">ocp-dev-vpc</span>\\n    - Subnet : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">Public subnet - zone a</span>\\n    - Auto assign public IP : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">Enable</span>\\n    - Security Group : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">select existing SG - BastionSG</span>\\n    - Storage : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">50GB</span>\\n    - **Launch Instance**\\n\\n# Cluster Installation\\n\\n> [Reference 1](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/networking/accessing-hosts#accessing-hosts-on-aws_accessing-hosts)\\n> [Reference 2](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/networking/accessing-hosts#accessing-hosts-on-aws_accessing-hosts)\\n\\n### A. **Tools**\\n\\n> [Link mirror client OCP](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/networking/accessing-hosts#accessing-hosts-on-aws_accessing-hosts)\\n\\n**\uD83D\uDCA1 Do this in Bastion Host with same AWS VPC**\\n\\n\\n\uD83D\uDCA1 Download the required tools for OCP cluster installation on the bastion host <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**after SSH access using the key**</span>. You\'ll need to download both the OC client (oc command) and OCP Cluster Installer (ocp install) that match your desired version.\\n\\n\\n![image.png](/notion-images/5754dd685b616c4e8dd19888f17285888a8f320e.png)\\n\\n1. Setup <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`OC Command`**</span>\\n\\n    ```bash\\n    mkdir {ocp-dev,tools}\\n    cd tools\\n    wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.15.35/openshift-client-linux-4.15.35.tar.gz\\n    tar xvf openshift-client-linux-4.15.35.tar.gz\\n    sudo cp kubectl /usr/bin/\\n    sudo cp oc /usr/bin/\\n    ```\\n\\n2. Download <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`OCP Installer`**</span>\\n\\n    ```bash\\n    wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.15.35/openshift-install-linux-4.15.35.tar.gz\\n    tar xvf openshift-install-linux-4.15.35.tar.gz\\n    cp ~/tools/openshift-install ~/ocp-dev/\\n    ```\\n\\n3. Config <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`OC Bash Completion`**</span>\\n\\n    ```bash\\n    oc completion bash > oc_bash_completion\\n    sudo cp oc_bash_completion /etc/bash_completion.d/\\n    ```\\n\\n\\n### B. **SSH Key**\\n\\n> [Reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/installing_on_any_platform/installing-platform-agnostic#ssh-agent-using_installing-platform-agnostic)\\n\\n\uD83D\uDCA1 The SSH key will be used to establish communication between the installer <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`Bastion Host`**</span> and the `Bootstrap node` to create the cluster according to the configuration \\n\\n\\n![image.png](/notion-images/68468d225fbeb8c8fd16a5ef4486499366a63a90.png)\\n\\n1. Generate <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`SSH key`**</span>\\n\\n    ```bash\\n    ssh-keygen -t ed25519 -N \'\' -f <path>/<file_name>\\n    ls ~/.ssh\\n    cat <path>/<file_name>.pub\\n    ```\\n\\n2. Add <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`ssh-agent`**</span>\\n\\n    ```bash\\n    eval \\"$(ssh-agent -s)\\"\\n    ssh-add <path>/<file_name>\\n    ```\\n\\n\\n### C. **Pull Secret**\\n\\n> [Reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/installing_on_any_platform/installing-platform-agnostic#ssh-agent-using_installing-platform-agnostic)\\n> [Reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/installing_on_any_platform/installing-platform-agnostic#ssh-agent-using_installing-platform-agnostic)\\n\\n\uD83D\uDCA1 A pull secret is used to identify users who want to use OCP. The pull secret is obtained from the Red Hat user who will create the cluster, and logging in is required to obtain it.\\n\\n1. <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`Login`**</span> **→** <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`Downloads`**</span> **→** <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`Tokens`**</span> **-** Copy <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`Pull Secret`**</span>\\n\\n    ![image.png](/notion-images/7dbc8e2d062c87ea43db0b3b02af718ca465aca6.png)\\n\\n\\n### **D. Install Config** \\n\\n\\n\uD83D\uDCA1 Creating installation configuration for AWS OCP cluster through manifest generation\\n\\n1. Create Installer Configuration\\n\\n    ![image.png](/notion-images/30dc1b6a81c7a7bc4928e1c35f74b116833e022b.png)\\n\\n\\n    ```bash\\n    ./openshift-install create install-config --dir <installation_directory>\\n    ```\\n\\n    - SSH Public Key : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">/home/ubuntu/.ssh/ocp-dev.pub</span>\\n    - Platform : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">AWS</span>\\n    - AWS Access Key ID : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">(Access key ID)</span>\\n    - AWS Secret Access Key : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">(Secret Access Key)</span>\\n    - Region : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">ap-southeast-1</span>\\n    - Base Domain : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">ziddma.my.id</span>\\n    - Cluster Name : <span class=\\"notion-red\\" style=\\"color:#E03A45\\">ocp-dev</span>\\n    - Pull Secret : **(Paste Pull Secret)**\\n2. Edit Install Config\\n    > [Link Install-config Example](https://raw.githubusercontent.com/Ziddma/Openshift-AWS/refs/heads/main/install/install-config.yaml)\\n\\n    \uD83D\uDCA1 Edit install-config.yaml allow us to customize cluster’s specification. This include change detail such the instance type for Master and Worker node, the worker replica count and disk size for each node\\n\\n\\n    ![image.png](/notion-images/bd708be093f96b7bcad81c569f2539d9fb42fc63.png)\\n\\n\\n    ![5f626604-5708-4096-ab4b-f7b92b8c9331.png](/notion-images/0b3e30127072fdaeb085faa2bf40702e6acc20aa.png)\\n\\n\\n    \uD83D\uDCA1Installing OCP has standard minimum specification instance on AWS, 4/16 for **Master Node** and 2/8 for **Worker node**, and each type at least have 100gb storage for running system. In this lab we use M5.* family instance type and many tested type for installing OCP in below references\\n\\n    > [Instance Type Specification](https://aws.amazon.com/ec2/instance-types/)\\n    > [Reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/installing_on_aws/installer-provisioned-infrastructure)\\n    - Master\\n        - **Iops : 4000**\\n        - **Size : 100**\\n        - **Type m5.2xlarge**\\n    - Compute\\n        - **Iops : 4000**\\n        - **Size : 100**\\n        - **Type m5.4xlarge**\\n    - Subnet : (Paste all subnet ID from AWS console)\\n    - Publish : **External / Internal**\\n3. Create manifest from <span class=\\"notion-red\\" style=\\"color:#E03A45\\">**`install-config`**</span>\\n\\n    ![image.png](/notion-images/db0ad3dc17aa6741a5373e736a0d856c6374d48a.png)\\n\\n\\n    ```bash\\n    ./openshift-install create manifests --dir <installation_directory>\\n    ```\\n\\n\\n### E. **Create Cluster**\\n\\n\\n\uD83D\uDCA1 When create cluster it happens any time your connection can distraction, either we can use tmux or screen to prevent \\n\\n\\n```bash\\ntmux new -s <session_name>\\n\\t\\t\\n#If lost connection, resume with\\ntmux list-session\\ntmux a -t <session_name>\\n```\\n\\n1. Create Cluster from manifest\\n\\n    ```bash\\n    ./openshift-install create cluster --dir <installation_directory> --log-level=debug\\n    ```\\n\\n\\n    \uD83D\uDCA1 During installation, cluster gradually from Master - Bootstrap - Worker nodes, this can be monitoring from aws console, eventually cluster has successfully made, log installation can check at :\\n\\n\\n    ```bash\\n    tail -f ~/<installation_directory>/.openshift-install.log\\n    ```\\n\\n2. Verification Installation\\n    > [Reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/validation_and_troubleshooting/validating-an-installation#reviewing-the-installation-log_validating-an-installation)\\n\\n    \uD83D\uDCA1 Verification cluster to convincing all cluster setup and bootstrap are clearly done\\n\\n\\n    ![c1c43e4e-372e-4541-8ac3-6822459147ff.png](/notion-images/e5070b24fedb31ea6e843c8ef37a3a68984ebf88.png)\\n\\n\\n    Link of web console & password will appear when all part cluster done made, password kube admin can be found at\\n\\n\\n    ```bash\\n    ~/<Installation_folder>/auth/kubeadmin-password\\n    ```\\n\\n\\n    To make bastion can access with role **system:admin**, use command\\n\\n\\n    ```bash\\n    export KUBECONFIG=<Installation_folder>/auth/kubeconfig\\n    ```\\n\\n\\n    Logging OC CLI can be done through the web console, which require being logged in as an administrator first\\n\\n\\n    **Login → Copy login command → Display Token → Copy token\xa0 → Paste in bastion**\\n\\n\\n    ![image.png](/notion-images/9266a721af06f0391f8fe9bd7f381d3c26c5bfd6.png)\\n\\n\\n![image.png](/notion-images/26c39956f73be185810d99d71984980623455850.png)\\n\\n","tags":["Cloud","DevOps","Monitoring"],"category":"Openshift","headings":[{"level":1,"text":"Topology","slug":"topology"},{"level":1,"text":"Prerequisite","slug":"prerequisite"},{"level":3,"text":"A. DNS Record","slug":"a-dns-record"},{"level":3,"text":"B. VPC","slug":"b-vpc"},{"level":3,"text":"C. Security Group","slug":"c-security-group"},{"level":3,"text":"D. Bastion","slug":"d-bastion"},{"level":1,"text":"Cluster Installation","slug":"cluster-installation"},{"level":3,"text":"A. Tools","slug":"a-tools"},{"level":3,"text":"B. SSH Key","slug":"b-ssh-key"},{"level":3,"text":"C. Pull Secret","slug":"c-pull-secret"},{"level":3,"text":"D. Install Config","slug":"d-install-config"},{"level":3,"text":"E. Create Cluster","slug":"e-create-cluster"}],"status":"Published"},{"id":"27e4a369-535d-8099-ad66-d054592ff21e","title":"High Availibility Web Server using Nginx and Keepalived","slug":"high-availibility-web-server-using-nginx-and-keepalived","coverImage":"/notion-images/ef4fde22b740edc5070330a15671d4f6cffc5409.png","coverImageOriginal":"/notion-images/ef4fde22b740edc5070330a15671d4f6cffc5409.png","description":"Build a reliable web server setup with Nginx and Keepalived to achieve High Availability (HA) ensuring your website stays online even if one server fails.","date":"2024-06-13","content":"\\n\\nBuild a reliable web server setup with Nginx and Keepalived to achieve High Availability (HA) ensuring your website stays online even if one server fails.\\n\\n\\n## What is High Availibility?\\n\\n\\nHigh Availability Web Server is the ability of a web server system to continue operating and providing uninterrupted services around the clock, including when there is a failure or outage of one of its components. The main purpose of high availability (HA) is to reduce the risk of downtime or time when services are not available.\\n\\n\\nOn this occasion, I will give a little example of implementing HA web server only on a virtual machine (VM) using a virtual box, now here is a little description of the needs for implementing the High Availibility web server\\n\\n\\n### Requirements\\n\\n\\n| No | Number | Type of Needs | Tools                  | Description                                                                            |\\n| -- | ------ | ------------- | ---------------------- | -------------------------------------------------------------------------------------- |\\n| 1  | 2      | Web Server    | 1. Apache2             | Used to serve requests from clients and process all processing                         |\\n| 2  | 1      | Master LB     | 1. Nginx 2. Keepalived | Used as the main load balancer which will later be used to distribute _traffic_        |\\n| 3  | 1      | Backup LB     | 1. Nginx 2. Keepalived | Used as a backup load balancer which will _take over_ if the main load balancer fails. |\\n\\n\\n### Topology\\n\\n\\n![Untitled.png](/notion-images/00d43376c6ac0e96177f9ea00df7e6ef448ee2ec.png)\\n\\n\\n| Node Name                | IP             |\\n| ------------------------ | -------------- |\\n| Master Load Balancer     | 192.168.100.10 |\\n| Backup Load Balancer     | 192.168.100.11 |\\n| Web Server 1             | 192.168.100.12 |\\n| Web Server 2             | 192.168.100.13 |\\n| Virtual IP Load Balancer | 192.168.100.50 |\\n\\n\\n## Installation\\n\\n\\n### A. Virtual Box\\n\\n\\n![Untitled.png](/notion-images/29cf3b8fba78b01781f09eb6626d9665a4eb4703.png)\\n\\n\\n### B. Network Setting \\n\\n\\nHas 2 adapters that are active **NAT** and **Host-only Adapter**\\n\\n\\n![Untitled.png](/notion-images/a1fcfd1fb0a2a757aee88b5c82bfb445806c9d38.png)\\n\\n\\n![Untitled.png](/notion-images/2625ae1b10d66840bb7c78b4e044ed3761c2b8b6.png)\\n\\n\\n![Untitled.png](/notion-images/c5ea140900bd74a956b463442d36ab9fab0af0fe.png)\\n\\n\\n### C. Assign IP\\n\\n1. Assign IP using ifupdown using the command\\n\\n    ```bash\\n    sudo apt install ifupdown -y\\n    ```\\n\\n2. Then enter the IP configuration\\n\\n    ```bash\\n    sudo nano /etc/network/interfaces\\n    ```\\n\\n3. Then change the contents of the configuration file\\n\\n    ```bash\\n    auto lo\\n    iface lo inet loopback\\n    \\n    auto enp0s3\\n    iface enp0s3 inet dhcp\\n    \\n    auto enp0s8\\n    iface enp0s8 inet static\\n    \\t__NCOLOR_START:red__address 192.168.100.10/24__NCOLOR_END__\\n    ```\\n\\n    - In the <span class=\\"notion-red\\" style=\\"color:#E03A45\\">red</span> section, replace it according to the IP that has been previously determined on all servers\\n\\n        ```bash\\n        systemctl restart networking\\n        ```\\n\\n4. **Perform IP configuration on all nodes that have been installed**\\n5. IP results using the **ip a** command on the Load Balancer Master node\\n\\n    ![Untitled.png](/notion-images/43e644dfa22217149a62dc291f54b14289c638db.png)\\n\\n\\n### D. Install Load Balancer\\n\\n1. Install nginx using the command\\n\\n    ```bash\\n    sudo apt install nginx -y\\n    ```\\n\\n2. Then enter the configuration for the proxy load balancer and create a file called \\"**sister.conf**\\"\\n\\n    ```bash\\n    sudo nano /etc/nginx/conf.d/sister.conf\\n    ```\\n\\n3. Enter the following configuration\\n\\n    ```bash\\n    upstream sister-lb {\\n    \\t__NCOLOR_START:red__server 192.168.100.12__NCOLOR_END__:80;\\n    \\t__NCOLOR_START:red__server 192.168.100.13__NCOLOR_END__:80;\\n    }\\n    server {\\n    \\tlisten 80;\\n    \\tserver_name sister.com;\\n    \\taccess_log /var/log/nginx/access.log;\\n    \\terror_log /var/log/nginx/error.log;\\n    location / {\\n    \\t\\tproxy_pass http://sister-lb;\\n    \\t\\tproxy_set_header Host $host;\\n    \\t\\tproxy_set_header X-Real-IP $remote_addr;\\n    \\t}\\n    }\\n    ```\\n\\n4. Second configuration in the /etc/nginx/nginx.conf folder\\n\\n    ```bash\\n    sudo nano /etc/nginx/nginx/conf\\n    ```\\n\\n5. Then use the config and paste it in the file\\n\\n    ```bash\\n    user www-data;\\n    worker_processes auto;\\n    worker_rlimit_nofile 8192;\\n    pid /run/nginx.pid;\\n    \\n    events {\\n            worker_connections 4096;\\n    }\\n    \\n    http {\\n            include       mime.types;\\n            default_type  application/octet-stream;\\n    \\n            # Log Setting\\n            log_format complete \'\\\\$remote_addr - \\\\$remote_user [\\\\$time_local] \\"\\\\$request\\" \'\\n                    \'\\\\$status $body_bytes_sent \\"\\\\$http_referer\\" \'\\n                    \'\\"\\\\$http_user_agent\\" \\"\\\\$http_x_forwarded_for\\" \'\\n                    \'rt=\\\\$request_time \'\\n                    \'ua=\\"\\\\$upstream_addr\\" us=\\"\\\\$upstream_status\\" \'\\n                    \'ut=\\"\\\\$upstream_response_time\\" ul=\\"\\\\$upstream_response_length\\"\';\\n            access_log /var/log/nginx/access.log complete;\\n            error_log  /var/log/nginx/error.log warn;\\n    \\n            # Sending fille Optimization\\n            sendfile on;\\n            tcp_nopush on;\\n            tcp_nodelay on;\\n    \\n            # Keepalive Connection\\n            keepalive_timeout 65;\\n    \\n            upstream sister-lb {\\n                    __NCOLOR_START:red__server 192.168.100.12:80;\\n                    server 192.168.100.13:80;__NCOLOR_END__\\n            }\\n    \\n            server {\\n                    listen 80;\\n                    server_name __NCOLOR_START:red__sister.com__NCOLOR_END__;\\n    \\n                    location / {\\n                            # HTTP 1.1\\n                            proxy_http_version 1.1;\\n                            proxy_set_header Connection \\"\\";\\n                            proxy_set_header Host $host;\\n                            proxy_pass http://__NCOLOR_START:red__sister-lb__NCOLOR_END__;\\n                    }\\n            }\\n    }\\n    ```\\n\\n    - In the upstream section filled with IP web server 1 with ip **100.12** and Web server 2 with ip = 100**.13**\\n6. Then restart nginx\\n\\n    ```bash\\n    sudo systemctl restart nginx\\n    ```\\n\\n7. Then run the following command, to check the configuration\\n\\n    ```bash\\n    sudo nginx -t\\n    ```\\n\\n    - If the configuration is successful, the results are as follows\\n\\n    ![Untitled.png](/notion-images/b8889cdc20c0aba531775fb3faded1cd3b01c484.png)\\n\\n8. Next we reload nginx so that the config runs\\n\\n    ```bash\\n    sudo nginx -s reload\\n    ```\\n\\n\\n### E. Install Web Server \\n\\n1. First install apache2 with the command\\n\\n    ```bash\\n    sudo apt install apache2 -y\\n    ```\\n\\n2. Then we will create a \\"**sister\\"** folder by way of\\n\\n    ```bash\\n    sudo mkdir –p /var/www/sister\\n    ```\\n\\n3. Create an index file for Web-Server-1 by way of\\n\\n    ```bash\\n    sudo nano /var/www/sister/index.html\\n    ```\\n\\n    - Fill in the text \\"This is web server 1\\" as follows\\n\\n    ![Untitled.png](/notion-images/456ab6bc90dd8e0e30c48499634194b516f77a92.png)\\n\\n    - Then save\\n4. Then create a new virtual host by way of :\\n\\n    ```bash\\n    sudo nano /etc/apache2/sites-available/000-default.conf\\n    ```\\n\\n    - Add a configuration line to the existing one\\n\\n    ```bash\\n    <VirtualHost *:80>\\n    ...\\n    \\tServerName __NCOLOR_START:red__sister.com__NCOLOR_END__\\n    \\t...\\n    \\tDocumentRoot __NCOLOR_START:red__/var/www/sister__NCOLOR_END__\\n    \\t__NCOLOR_START:red__<Directory /var/www/sister>\\n    \\t\\tOptions All\\n    \\t\\tAllowOverride All\\n    \\t\\tRequire all granted\\n    \\t</Directory>__NCOLOR_END__\\n    \\tErrorLog ${APACHE_LOG_DIR}/__NCOLOR_START:red__sister__NCOLOR_END__-error.log\\n    \\tCustomLog ${APACHE_LOG_DIR}/__NCOLOR_START:red__sister__NCOLOR_END__-access.log combined\\n    ...\\n    </VirtualHost *:80>\\n    ```\\n\\n    - So that it is more or less as follows\\n\\n    ![Untitled.png](/notion-images/41213f7a4635ab59bd0b1ecf7bc4bb4b359dd917.png)\\n\\n5. Restart the Apache Webserver by way of\\n\\n    ```bash\\n    sudo systemctl restart apache2\\n    ```\\n\\n6. **DO THE ABOVE STEPS ON WEB SERVER 2 AS WELL**\\n7. Check the web server using the ip in the web browser\\n    - Results\\n\\n    ![Untitled.png](/notion-images/acffe197364392236d4989e6e669429d25ba2b5d.png)\\n\\n\\n### F. Configuring Load Balancer\\n\\n1. First we will install keepalived with the command\\n\\n    ```bash\\n    sudo apt install keepalived -y\\n    ```\\n\\n2. Then we will create the keepalived.conf file in the specified folder with the command\\n\\n    ```bash\\n    sudo nano /etc/keepalived/keepalived.conf\\n    ```\\n\\n3. There are 2 configurations on the **MASTER** and **BACKUP** Load balancer servers\\n    - **Master Server Load Balancer**\\n\\n    ```yaml\\n    vrrp_instance VI_1 {\\n            interface __NCOLOR_START:red__enp0s8__NCOLOR_END__\\n            state __NCOLOR_START:red__MASTER__NCOLOR_END__\\n            priority 200\\n            advert_int 1\\n            unicast_src_ip __NCOLOR_START:red__192.168.100.10__NCOLOR_END__\\n            unicast_peer {\\n                    __NCOLOR_START:red__192.168.100.11__NCOLOR_END__\\n            }\\n    \\n            virtual_router_id 33\\n            virtual_ipaddress {\\n                    __NCOLOR_START:red__192.168.100.50/24__NCOLOR_END__\\n            }\\n    \\n            authentication {\\n                    auth_type PASS\\n                    auth_pass 123\\n            }\\n    }\\n    ```\\n\\n    - **Backup Server Load Balancer**\\n\\n    ```yaml\\n    vrrp_instance VI_1 {\\n            interface __NCOLOR_START:red__enp0s8__NCOLOR_END__\\n            state __NCOLOR_START:red__BACKUP__NCOLOR_END__\\n            priority 100\\n            advert_int 1\\n            unicast_src_ip __NCOLOR_START:red__192.168.100.11__NCOLOR_END__\\n            unicast_peer {\\n                    __NCOLOR_START:red__192.168.100.10__NCOLOR_END__\\n            }\\n    \\n            virtual_router_id 33\\n            virtual_ipaddress {\\n                    __NCOLOR_START:red__192.168.100.50/24__NCOLOR_END__\\n            }\\n    \\n            authentication {\\n                    auth_type PASS\\n                    auth_pass 123\\n            }\\n    }\\n    ```\\n\\n    - Note:\\n        1. <span class=\\"notion-red\\" style=\\"color:#E03A45\\">interface</span> according to the interface that we install on the VM\\n        2. <span class=\\"notion-red\\" style=\\"color:#E03A45\\">state</span> in the declaration according to Backup / Master\\n        3. <span class=\\"notion-red\\" style=\\"color:#E03A45\\">Priority</span> on Master must be higher than Backup\\n        4. <span class=\\"notion-red\\" style=\\"color:#E03A45\\">unicast_src_ip</span>: according to the IP of the node itself\\n        5. <span class=\\"notion-red\\" style=\\"color:#E03A45\\">unicast_peer</span>: according to the backup IP or vice versa\\n        6. <span class=\\"notion-red\\" style=\\"color:#E03A45\\">virtual_ipaddress</span>: according to the VIP that we created earlier\\n    - Here is the Load Balancer Master config\\n\\n    ![Untitled.png](/notion-images/365fdb58f00a4a60d3b8a94b0ded54ac3c80022e.png)\\n\\n4. Then **restart** **keepalived** on both servers with the command\\n\\n    ```bash\\n    systemctl restart keepalived\\n    ```\\n\\n5. Check the keepalived status with the command\\n\\n    ```bash\\n    systemctl status keepalived\\n    ```\\n\\n    - Here\'s an example of keepalived status on the Master Load Balancer server\\n\\n    ![Untitled.png](/notion-images/94fe4fb21c48772da5f8fcb5d591132159121ee9.png)\\n\\n\\n## Verify\\n\\n\\n### A. Proving HA Web server 1 and 2 \\n\\n1. By accessing the load balancer ip or domain that we have set and **refreshing** the **page** repeatedly2\\n    1. Can the web server be accessed by both?\\n    2. If web server 1 dies, can only web server 2 be displayed?\\n    3. If web server 2 dies, only web server 1 can be displayed?\\n\\n![Untitled.png](/notion-images/256b7caa006bc22b3d0e32abe7b2ea772eb511b5.png)\\n\\n1. By accessing the logs from the load balancer itself, but to make it clear we will install \\"**ccze**\\" on the LB Master with the command\\n\\n    ```bash\\n    apt install ccze\\n    ```\\n\\n    - Then access the log in the **/var/log/nginx/access.log** folder with the command\\n\\n    ```bash\\n    tail -f /var/log/nginx/access.log | ccze\\n    ```\\n\\n    - Then try to refresh the Virtual IP repeatedly, here is an example\\n\\n    ![Untitled.png](/notion-images/46a8887a1f26b4ce4ade6fbdf66724d754c6457c.png)\\n\\n    - Pay attention to the red circle\\n        1. Is the IP generated alternately between Web servers 1 and 2?\\n        2. If one of the Web IPs is turned off, will there only be 1 ip that is on and vice versa?\\n\\n### B. Proof of HA LB Master and LB Backup \\n\\n1. In testing the Load Balancer here, we will try to turn off 1 LB master, then check the keepalived status on the LB master or LB backup.\\n\\n    ```bash\\n    systemctl status keepalived\\n    ```\\n\\n    - Here is an example of the output of the command on **LB Master**\\n\\n    ![Untitled.png](/notion-images/c073445f8d560a578cd4eb9599784117a574d0f8.png)\\n\\n    - Note on the red circled one, that the LB master shows the master state\\n    - Then is an example of output from **LB Backup**\\n\\n    ![Untitled.png](/notion-images/00055495973f9fd14b3b922a1c4b4975f559dfd5.png)\\n\\n    - **Then we will try to test by entering down time on LB Master with the command to turn off the node or turn off keepalived on LB Master.**\\n        1. Does the LB backup change status to \\"Entering MASTER STATE\\"?\\n        2. If the LB master server enters uptime, does the status on keepalived become \\"Entering MASTER STATE\\"?\\n        3. Does the LB backup change back to \\"BACKUP STATE\\"?\\n","tags":["Networking","Linux","Infra"],"category":"RHEL","headings":[{"level":2,"text":"What is High Availibility?","slug":"what-is-high-availibility"},{"level":3,"text":"Requirements","slug":"requirements"},{"level":3,"text":"Topology","slug":"topology"},{"level":2,"text":"Installation","slug":"installation"},{"level":3,"text":"A. Virtual Box","slug":"a-virtual-box"},{"level":3,"text":"B. Network Setting","slug":"b-network-setting"},{"level":3,"text":"C. Assign IP","slug":"c-assign-ip"},{"level":3,"text":"D. Install Load Balancer","slug":"d-install-load-balancer"},{"level":3,"text":"E. Install Web Server","slug":"e-install-web-server"},{"level":3,"text":"F. Configuring Load Balancer","slug":"f-configuring-load-balancer"},{"level":2,"text":"Verify","slug":"verify"},{"level":3,"text":"A. Proving HA Web server 1 and 2","slug":"a-proving-ha-web-server-1-and-2"},{"level":3,"text":"B. Proof of HA LB Master and LB Backup","slug":"b-proof-of-ha-lb-master-and-lb-backup"}],"status":"Published"}]')}};